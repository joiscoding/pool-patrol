#!/usr/bin/env python3
"""Test script for the Case Manager agent.

This script tests the Case Manager agent's orchestration capabilities:
- Verification flow (all pass â†’ verified)
- Failure flow (fail â†’ open case â†’ outreach)
- Re-verification after employee update
- Timeout flow (1 week â†’ cancel_membership HITL)
- Agent explanation capabilities

Run from the project root:

    poetry run python tests/test_case_manager.py

Make sure you have:
1. Run the database seed: bun run db:seed (or npx prisma db seed)
2. Set OPENAI_API_KEY in your environment
"""

import os
import sys
from datetime import datetime, timedelta, timezone
from functools import wraps
from pathlib import Path
from unittest.mock import MagicMock

# Add packages to path for development
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root / "packages"))

from dotenv import load_dotenv

# Load environment variables
load_dotenv(project_root / ".env", override=True)

# Configure LangSmith tracing
from agents.utils import configure_langsmith
langsmith_enabled = configure_langsmith()

# Import modules under test
from agents.case_manager import (
    check_timeout,
    create_case_manager_agent,
    get_existing_case,
    investigate_vanpool_sync,
    parse_case_manager_result,
    PreloadedContext,
    _build_config,
    _preload_investigation_context,
)
from agents.structures import CaseManagerRequest, CaseManagerResult
from core.database import reset_engine


# =============================================================================
# Test Utilities
# =============================================================================


def print_header(title: str) -> None:
    """Print a formatted test header."""
    print("\n" + "=" * 60)
    print(title)
    print("=" * 60)


def print_subheader(title: str) -> None:
    """Print a formatted sub-header."""
    print(f"\n--- {title} ---")


class AssertionError(Exception):
    """Raised when a test assertion fails."""
    pass


def check(condition: bool, success_msg: str, fail_msg: str) -> None:
    """Assert a condition and print result. Raises AssertionError on failure."""
    if condition:
        print(f"   âœ“ {success_msg}")
    else:
        print(f"   âœ— {fail_msg}")
        raise AssertionError(fail_msg)


def check_equal(actual, expected, field_name: str) -> None:
    """Assert equality and print result."""
    check(
        actual == expected,
        f"{field_name} is '{actual}'",
        f"Expected {field_name} '{expected}', got '{actual}'",
    )


def check_contains(haystack: str, needle: str, context: str) -> None:
    """Assert string contains substring."""
    check(
        needle in haystack,
        f"{context} contains '{needle}'",
        f"{context} should contain '{needle}'",
    )


def check_is_instance(obj, cls, context: str) -> None:
    """Assert object is instance of class."""
    check(
        isinstance(obj, cls),
        f"{context} is {cls.__name__}",
        f"{context} should be {cls.__name__}, got {type(obj).__name__}",
    )


def requires_api_key(func):
    """Decorator that skips test if OPENAI_API_KEY is not set."""
    @wraps(func)
    def wrapper(*args, **kwargs):
        if not os.environ.get("OPENAI_API_KEY"):
            print("\nâš  OPENAI_API_KEY not set. Skipping this test.")
            return None
        return func(*args, **kwargs)
    return wrapper


def print_investigation_result(result) -> None:
    """Print a CaseManagerResult in a formatted way."""
    print(f"   Vanpool ID: {result.vanpool_id}")
    print(f"   Case ID: {result.case_id or 'None (no case needed)'}")
    print(f"   Outcome: {result.outcome}")
    print(f"   HITL Required: {result.hitl_required}")
    
    if result.reasoning:
        print_subheader("Reasoning")
        # Truncate long reasoning
        reasoning = result.reasoning[:200] + "..." if len(result.reasoning) > 200 else result.reasoning
        print(f"   {reasoning}")

    if result.outreach_summary:
        print_subheader("Outreach Summary")
        print(f"   {result.outreach_summary}")


# =============================================================================
# Unit Tests (Helper Functions)
# =============================================================================


def test_get_existing_case():
    """Test the get_existing_case helper function."""
    print_header("Testing get_existing_case helper")

    print("\n1. Looking up case for VP-105 (should have CASE-001)...")
    case = get_existing_case("VP-105")
    if case is not None:
        print(f"   âœ“ Found case: {case.get('case_id')} (status: {case.get('status')})")
    else:
        print("   ~ No case found (case might be resolved/cancelled)")

    print("\n2. Looking up case for VP-101 (should have no open case)...")
    no_case = get_existing_case("VP-101")
    if no_case is None:
        print("   âœ“ No open case found (as expected)")
    else:
        print(f"   ~ Found case: {no_case.get('case_id')} - may need to verify status")

    print("\nâœ“ get_existing_case helper working correctly!")
    return True


def test_check_timeout():
    """Test the check_timeout helper function."""
    print_header("Testing check_timeout helper")

    try:
        print("\n1. Testing with None case...")
        check_equal(check_timeout(None), False, "Timeout for None")

        print("\n2. Testing with recent case (2 days old)...")
        recent_case = {
            "case_id": "TEST-RECENT",
            "created_at": (datetime.now(timezone.utc) - timedelta(days=2)).isoformat(),
        }
        check_equal(check_timeout(recent_case), False, "Timeout for recent case")

        print("\n3. Testing with old case (10 days old)...")
        old_case = {
            "case_id": "TEST-OLD",
            "created_at": (datetime.now(timezone.utc) - timedelta(days=10)).isoformat(),
        }
        check_equal(check_timeout(old_case), True, "Timeout for old case")

        print("\nâœ“ check_timeout helper working correctly!")
        return True
    except AssertionError:
        return False


def test_parse_case_manager_result():
    """Test the result parsing function."""
    print_header("Testing parse_case_manager_result")

    try:
        print("\n1. Testing with valid JSON response...")
        valid_response = {
            "messages": [
                MagicMock(content='{"vanpool_id": "VP-101", "case_id": null, "outcome": "verified", "reasoning": "All checks passed", "hitl_required": false}')
            ]
        }
        result = parse_case_manager_result(valid_response, "VP-101", None)
        check_is_instance(result, CaseManagerResult, "Result")
        check_equal(result.outcome, "verified", "Outcome")
        print(f"   Reasoning: {result.reasoning}")

        print("\n2. Testing with empty messages...")
        empty_response = {"messages": []}
        result = parse_case_manager_result(empty_response, "VP-101", None)
        check_equal(result.outcome, "pending", "Outcome for empty messages")

        print("\nâœ“ parse_case_manager_result working correctly!")
        return True
    except AssertionError:
        return False


# =============================================================================
# Integration Tests (With Mocked Specialists)
# =============================================================================


@requires_api_key
def test_verification_pass_flow():
    """Test that verification passing returns 'verified' without opening a case."""
    print_header("Testing Verification Pass Flow")

    print("\n1. Investigating VP-101 (all employees should pass)...")
    print("   This calls the real Case Manager agent (may take a moment)...\n")

    request = CaseManagerRequest(vanpool_id="VP-101")
    result = investigate_vanpool_sync(request)

    print_investigation_result(result)

    # For a passing verification, we expect:
    # - outcome = "verified" (no case needed)
    # - case_id = None (no case was created)
    if result.outcome == "verified":
        print("\n   âœ“ Agent correctly returned 'verified' outcome")
    elif result.outcome == "pending":
        print("\n   ~ Agent returned 'pending' (might need human intervention)")
    else:
        print(f"\n   ~ Got outcome '{result.outcome}' - may have existing case")

    print("\nâœ“ Verification pass flow test completed!")
    return True


@requires_api_key
def test_verification_fail_with_mock():
    """Test that verification failure opens a case (documented behavior)."""
    print_header("Testing Verification Fail Flow (Documented)")

    print("\n   Expected behavior when shift fails:")
    print("   1. Agent calls upsert_case() with reason")
    print("   2. Agent calls run_outreach() to contact employee")
    print("   3. Returns outcome='pending' waiting for reply")
    print("\n   Note: Full mock test requires dependency injection in tools")

    print("\nâœ“ Verification fail flow documented!")
    return True


def test_agent_creation():
    """Test that the agent can be created without errors."""
    print_header("Testing Agent Creation")

    print("\n1. Creating Case Manager agent...")
    try:
        agent = create_case_manager_agent()
        print(f"   âœ“ Agent created successfully (type: {type(agent).__name__})")
        return True
    except Exception as e:
        print(f"   âœ— Failed to create agent: {e}")
        return False


def test_preload_context():
    """Test that context preloading works correctly."""
    print_header("Testing Context Preloading")

    try:
        print("\n1. Preloading context for VP-101 (should succeed)...")
        ctx = _preload_investigation_context("VP-101")

        check(
            not isinstance(ctx, CaseManagerResult),
            "Got PreloadedContext (not error)",
            f"Should not get error result: {getattr(ctx, 'reasoning', '')}",
        )
        check_is_instance(ctx, PreloadedContext, "Context")
        print(f"   Vanpool ID: {ctx.vanpool_id}, Case ID: {ctx.case_id}")

        check_contains(ctx.message, "VP-101", "Message")
        check_contains(ctx.message, "upsert_case", "Message")

        print("\n2. Preloading context for non-existent vanpool...")
        ctx_err = _preload_investigation_context("VP-FAKE-999")

        check_is_instance(ctx_err, CaseManagerResult, "Error context")
        check_equal(ctx_err.outcome, "pending", "Error outcome")
        print(f"   Error reason: {ctx_err.reasoning[:50]}...")

        print("\nâœ“ Context preloading test passed!")
        return True
    except AssertionError:
        return False


def test_config_building():
    """Test that config building works correctly."""
    print_header("Testing Config Building")

    try:
        print("\n1. Building config without case ID...")
        config = _build_config("VP-101", None)

        check("configurable" in config, "Config has configurable section", "Missing configurable")
        thread_id = config["configurable"]["thread_id"]
        check(
            thread_id.startswith("investigation-"),
            f"Thread ID generated: {thread_id[:30]}...",
            "Thread ID should start with 'investigation-'",
        )

        print("\n2. Building config with case ID...")
        config = _build_config("VP-101", "CASE-123")

        check_equal(config["configurable"]["thread_id"], "CASE-123", "Thread ID")
        check_equal(config["metadata"]["vanpool_id"], "VP-101", "Metadata vanpool_id")

        print("\nâœ“ Config building test passed!")
        return True
    except AssertionError:
        return False


# =============================================================================
# Full Integration Test
# =============================================================================


@requires_api_key
def test_full_investigation():
    """Run a full investigation and report results."""
    print_header("Full Investigation Test")

    print("\n1. Running full investigation for VP-101...")
    print("   This is an end-to-end test with real LLM calls.\n")

    request = CaseManagerRequest(vanpool_id="VP-101")
    result = investigate_vanpool_sync(request)

    print_investigation_result(result)

    print("\nâœ“ Full investigation test completed!")
    return True


# =============================================================================
# Main
# =============================================================================


def main():
    """Run all Case Manager tests."""
    print_header("Pool Patrol - Case Manager Agent Test Suite")

    print("\nðŸ“‹ Testing Case Manager agent orchestration")
    print("   For tool-level tests, see: tests/test_case_manager_tools.py")

    # Reset database engine once at start
    reset_engine()

    results = {}

    # Unit tests (helper functions)
    print("\n\n" + "=" * 60)
    print("UNIT TESTS (Helper Functions)")
    print("=" * 60)

    results["get_existing_case"] = test_get_existing_case()
    results["check_timeout"] = test_check_timeout()
    results["parse_result"] = test_parse_case_manager_result()
    results["agent_creation"] = test_agent_creation()
    results["preload_context"] = test_preload_context()
    results["config_building"] = test_config_building()

    # Integration tests
    print("\n\n" + "=" * 60)
    print("INTEGRATION TESTS (Agent Flows)")
    print("=" * 60)

    results["verification_pass"] = test_verification_pass_flow()
    results["verification_fail_mock"] = test_verification_fail_with_mock()
    results["full_investigation"] = test_full_investigation()

    # Summary
    print_header("Test Summary")

    passed = 0
    failed = 0
    skipped = 0

    for name, result in results.items():
        if result is True:
            status = "âœ“ Pass"
            passed += 1
        elif result is False:
            status = "âœ— Fail"
            failed += 1
        else:
            status = "âš  Skipped"
            skipped += 1
        print(f"  {name}: {status}")

    print(f"\n  Total: {passed} passed, {failed} failed, {skipped} skipped")

    # Return exit code
    return 1 if failed > 0 else 0


if __name__ == "__main__":
    sys.exit(main())
